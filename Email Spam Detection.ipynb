{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["smux8GIhE9km"],"authorship_tag":"ABX9TyOBkUSIWxgs7hMwJI7oTjew"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#SMS Spam"],"metadata":{"id":"5uUsr_wcAgOj"}},{"cell_type":"markdown","source":["\n","\n","# --- Using Logistic Regression\n","\n"],"metadata":{"id":"smux8GIhE9km"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xXyhFPgi_CHc","executionInfo":{"status":"ok","timestamp":1754712357970,"user_tz":420,"elapsed":178,"user":{"displayName":"Devika Rudagi","userId":"17017947672779449369"}},"outputId":"22b41b3f-9288-49f3-814f-d52c0a50855e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9524663677130045\n","              precision    recall  f1-score   support\n","\n","           0       0.95      1.00      0.97       965\n","           1       0.97      0.67      0.79       150\n","\n","    accuracy                           0.95      1115\n","   macro avg       0.96      0.83      0.88      1115\n","weighted avg       0.95      0.95      0.95      1115\n","\n","Congratulations! You've won a free ticket to the Bahamas. Claim now! -> Spam\n","Hi Sarah, are we still on for the meeting tomorrow? -> Ham\n"]}],"source":["# Step 1: Import libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Step 2: Load dataset\n","# This example uses the SMS Spam Collection dataset\n","df = pd.read_csv(\"/content/sample_data/spam.csv\", encoding='latin-1')[['v1', 'v2']]\n","df.columns = ['label', 'text']\n","\n","# Convert labels to binary (ham=0, spam=1)\n","df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n","\n","# Step 3: Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['text'], df['label'], test_size=0.2, random_state=42\n",")\n","\n","# Step 4: Text vectorization using TF-IDF\n","vectorizer = TfidfVectorizer(stop_words='english')\n","X_train_tfidf = vectorizer.fit_transform(X_train)\n","X_test_tfidf = vectorizer.transform(X_test)\n","\n","# Step 5: Train Logistic Regression model\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train_tfidf, y_train)\n","\n","# Step 6: Predictions & Evaluation\n","y_pred = model.predict(X_test_tfidf)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","\n","# Step 7: Test with custom emails\n","emails = [\n","    \"Congratulations! You've won a free ticket to the Bahamas. Claim now!\",\n","    \"Hi Sarah, are we still on for the meeting tomorrow?\"\n","]\n","emails_tfidf = vectorizer.transform(emails)\n","predictions = model.predict(emails_tfidf)\n","\n","for email, label in zip(emails, predictions):\n","    print(f\"{email} -> {'Spam' if label == 1 else 'Ham'}\")\n"]},{"cell_type":"markdown","source":["\n","\n","# --- Using Naive Bayes\n","\n"],"metadata":{"id":"Ky_jQH1uFE64"}},{"cell_type":"code","source":["#Step 1 – Import libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","#Step 2 – Load and explore the dataset\n","# Example: SMS spam dataset\n","df = pd.read_csv(\"/content/sample_data/spam.csv\", encoding='latin-1')[['v1', 'v2']]\n","df.columns = ['label', 'text']\n","df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n","print(df.head())\n","\n","#Step 3 – Split into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['text'], df['label'], test_size=0.2, random_state=42)\n","\n","#Step 4 – Convert text into numerical features\n","#We use TF-IDF (Term Frequency – Inverse Document Frequency).\n","vectorizer = TfidfVectorizer(stop_words='english')\n","X_train_tfidf = vectorizer.fit_transform(X_train)\n","X_test_tfidf = vectorizer.transform(X_test)\n","\n","#Step 5 – Train the model\n","#We’ll use Multinomial Naive Bayes (fast & effective for text).\n","model = MultinomialNB()\n","model.fit(X_train_tfidf, y_train)\n","\n","#Step 6 – Evaluate\n","y_pred = model.predict(X_test_tfidf)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","\n","#Step 7 – Test with new emails\n","emails = [\n","    \"Congratulations! You've won a $1000 gift card. Click here to claim.\",\n","    \"Hi John, can we reschedule our meeting to tomorrow?\"\n","]\n","emails_tfidf = vectorizer.transform(emails)\n","predictions = model.predict(emails_tfidf)\n","\n","for email, label in zip(emails, predictions):\n","    print(f\"{email} -> {'Spam' if label == 1 else 'Ham'}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlxavKIdD9so","executionInfo":{"status":"ok","timestamp":1754713401207,"user_tz":420,"elapsed":96,"user":{"displayName":"Devika Rudagi","userId":"17017947672779449369"}},"outputId":"36a08c96-fc8a-4805-95a7-6b776209c027"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["   label                                               text\n","0      0  Go until jurong point, crazy.. Available only ...\n","1      0                      Ok lar... Joking wif u oni...\n","2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n","3      0  U dun say so early hor... U c already then say...\n","4      0  Nah I don't think he goes to usf, he lives aro...\n","Accuracy: 0.9668161434977578\n","              precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98       965\n","           1       1.00      0.75      0.86       150\n","\n","    accuracy                           0.97      1115\n","   macro avg       0.98      0.88      0.92      1115\n","weighted avg       0.97      0.97      0.96      1115\n","\n","Congratulations! You've won a $1000 gift card. Click here to claim. -> Spam\n","Hi John, can we reschedule our meeting to tomorrow? -> Ham\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"W_ZPCCT_8oaD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Using CNN"],"metadata":{"id":"t3rkRfeevew7"}},{"cell_type":"code","source":["# Step 1: Imports\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Step 2: Load dataset\n","df = pd.read_csv(\"/content/sample_data/spam.csv\", encoding='latin-1')[['v1', 'v2']]\n","df.columns = ['label', 'text']\n","df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n","\n","# Step 3: Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['text'], df['label'], test_size=0.2, random_state=42\n",")\n","\n","# Step 4: Tokenization\n","vocab_size = 5000\n","max_len = 100\n","tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_test_seq = tokenizer.texts_to_sequences(X_test)\n","\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n","X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n","\n","# Step 5: Build CNN model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 128, input_length=max_len)) # Increased embedding dimension\n","model.add(Conv1D(filters=128, kernel_size=5, activation='relu')) # Increased filters\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Dropout(0.5))\n","model.add(Conv1D(filters=128, kernel_size=5, activation='relu')) # Added another Conv1D layer and increased filters\n","model.add(MaxPooling1D(pool_size=2)) # Added another MaxPooling1D layer\n","model.add(Dropout(0.5))\n","model.add(Flatten())\n","model.add(Dense(64, activation='relu')) # Increased dense layer units\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Step 6: Train\n","history = model.fit(\n","    X_train_pad, y_train,\n","    epochs=5,\n","    batch_size=64,\n","    validation_data=(X_test_pad, y_test),\n","    verbose=2\n",")\n","\n","# Step 7: Evaluate\n","y_pred_cnn = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_cnn))\n","print(classification_report(y_test, y_pred_cnn))\n","\n","\n","# Step 8: Test with new emails\n","emails = [\n","    \"Congratulations! You've been selected to win a free trip to Paris. Click here to claim!\",\n","    \"Hi, just checking if we are still meeting for lunch tomorrow.\"\n","]\n","emails_seq = tokenizer.texts_to_sequences(emails)\n","emails_pad = pad_sequences(emails_seq, maxlen=max_len, padding='post')\n","\n","predictions = (model.predict(emails_pad) > 0.5).astype(\"int32\")\n","\n","for email, label in zip(emails, predictions):\n","    print(f\"{email} -> {'Spam' if label == 1 else 'Ham'}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"bc6xYzqzvdjU","executionInfo":{"status":"ok","timestamp":1754726921496,"user_tz":420,"elapsed":37608,"user":{"displayName":"Devika Rudagi","userId":"17017947672779449369"}},"outputId":"fc643d1a-e60f-410b-86d8-25ec7c80a58d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["70/70 - 8s - 115ms/step - accuracy: 0.8952 - loss: 0.2523 - val_accuracy: 0.9803 - val_loss: 0.0781\n","Epoch 2/5\n","70/70 - 6s - 83ms/step - accuracy: 0.9874 - loss: 0.0497 - val_accuracy: 0.9821 - val_loss: 0.0628\n","Epoch 3/5\n","70/70 - 6s - 88ms/step - accuracy: 0.9957 - loss: 0.0163 - val_accuracy: 0.9839 - val_loss: 0.0632\n","Epoch 4/5\n","70/70 - 10s - 150ms/step - accuracy: 0.9984 - loss: 0.0069 - val_accuracy: 0.9830 - val_loss: 0.0813\n","Epoch 5/5\n","70/70 - 6s - 88ms/step - accuracy: 1.0000 - loss: 4.2233e-04 - val_accuracy: 0.9857 - val_loss: 0.0748\n","\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","Accuracy: 0.9856502242152466\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       965\n","           1       0.97      0.93      0.95       150\n","\n","    accuracy                           0.99      1115\n","   macro avg       0.98      0.96      0.97      1115\n","weighted avg       0.99      0.99      0.99      1115\n","\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","Congratulations! You've been selected to win a free trip to Paris. Click here to claim! -> Spam\n","Hi, just checking if we are still meeting for lunch tomorrow. -> Ham\n"]}]}]}