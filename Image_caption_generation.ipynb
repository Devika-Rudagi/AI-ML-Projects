{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXChku7-57xC"
      },
      "source": [
        "#New model using InceptionV3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv3b_OMs_Ct2"
      },
      "source": [
        "#import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZMkMoytl9ERw",
        "outputId": "36f4d516-0233-4574-e806-29cd4c2735e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.74.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.14.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.12.0\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install pillow\n",
        "!pip install tqdm\n",
        "\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXnJHe1e99Qj"
      },
      "source": [
        "#Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUNmUthE97u7"
      },
      "outputs": [],
      "source": [
        "# Flickr8k Dataset (small, manageable)\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!unzip -q Flickr8k_Dataset.zip -d images/\n",
        "\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -q Flickr8k_text.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFOwue1T_Kks"
      },
      "source": [
        "#Load and Clean Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dFmIYaa-BA9",
        "outputId": "59bce1a6-dd18-4de0-c11b-74841a3d25ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample descriptions for image 1000268201_693b08cb0e:\n",
            "['startseq child in pink dress is climbing up set of stairs in an entry way endseq', 'startseq girl going into wooden building endseq', 'startseq little girl climbing into wooden playhouse endseq', 'startseq little girl climbing the stairs to her playhouse endseq', 'startseq little girl in pink dress going into wooden cabin endseq']\n"
          ]
        }
      ],
      "source": [
        "def load_doc(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def load_descriptions(doc):\n",
        "    descriptions = dict()\n",
        "    for line in doc.strip().split('\\n'):\n",
        "        tokens = line.split()\n",
        "        img_id, img_desc = tokens[0].split('.')[0], tokens[1:]\n",
        "        img_desc = ' '.join(img_desc)\n",
        "        if img_id not in descriptions:\n",
        "            descriptions[img_id] = []\n",
        "        descriptions[img_id].append(img_desc)\n",
        "    return descriptions\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            desc = desc_list[i].lower()\n",
        "            desc = desc.translate(table)\n",
        "            desc = desc.split()\n",
        "            desc = [word for word in desc if len(word) > 1 and word.isalpha()]\n",
        "            desc_list[i] = 'startseq ' + ' '.join(desc) + ' endseq'\n",
        "\n",
        "filename = 'Flickr8k.token.txt'\n",
        "doc = load_doc(filename)\n",
        "descriptions = load_descriptions(doc)\n",
        "clean_descriptions(descriptions)\n",
        "\n",
        "print(f\"Sample descriptions for image 1000268201_693b08cb0e:\")\n",
        "print(descriptions['1000268201_693b08cb0e'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syULHTcK_PVZ"
      },
      "source": [
        "#Extract Image Features Using Pretrained InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiqmN--4-FEl",
        "outputId": "1701c54c-e2ba-48c1-b864-2037b1a06f3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8091/8091 [52:23<00:00,  2.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted features for 8091 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def extract_features(directory):\n",
        "    model = InceptionV3(weights='imagenet')\n",
        "    model = Model(model.input, model.layers[-2].output)  # remove last layer\n",
        "    features = dict()\n",
        "    for img_name in tqdm(os.listdir(directory)):\n",
        "        img_path = os.path.join(directory, img_name)\n",
        "        img = load_img(img_path, target_size=(299,299))\n",
        "        x = img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = preprocess_input(x)\n",
        "        feature = model.predict(x, verbose=0)\n",
        "        img_id = img_name.split('.')[0]\n",
        "        features[img_id] = feature[0]\n",
        "    return features\n",
        "\n",
        "features = extract_features('images/Flicker8k_Dataset')\n",
        "print(f\"Extracted features for {len(features)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urK8KmeKJM6l"
      },
      "source": [
        "#Prepare Tokenizer and Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rKeo2-oIYwG",
        "outputId": "809665db-a74a-4d96-c25a-52415681f6da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 8766\n",
            "Maximum caption length: 34\n"
          ]
        }
      ],
      "source": [
        "# Flatten all captions\n",
        "all_captions = []\n",
        "for key in descriptions.keys():\n",
        "    all_captions.extend(descriptions[key])\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "max_length = max(len(desc.split()) for desc in all_captions)\n",
        "print(f\"Maximum caption length: {max_length}\")\n",
        "\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo_feature, vocab_size):\n",
        "    X1, X2, y = [], [], []\n",
        "    for desc in desc_list:\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            X1.append(photo_feature)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_ohs9AJOki"
      },
      "source": [
        "#Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpeSSrNqIbzI"
      },
      "outputs": [],
      "source": [
        "def data_generator(descriptions, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    keys = list(descriptions.keys())\n",
        "    n = len(keys)\n",
        "    while True:\n",
        "        for i in range(0, n, batch_size):\n",
        "            X1, X2, y = [], [], []\n",
        "            for key in keys[i:i+batch_size]:\n",
        "                photo_feature = features[key]\n",
        "                desc_list = descriptions[key]\n",
        "                in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo_feature, vocab_size)\n",
        "                for j in range(len(in_img)):\n",
        "                    X1.append(in_img[j])\n",
        "                    X2.append(in_seq[j])\n",
        "                    y.append(out_word[j])\n",
        "            yield [np.array(X1), np.array(X2)], np.array(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBLMrVvWJRca"
      },
      "source": [
        "Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QANGsH82I066"
      },
      "outputs": [],
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "    # Feature extractor (image input)\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    # Sequence input (caption input)\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    # Decoder (combine)\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "model = define_model(vocab_size, max_length)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qknzgAdJULs"
      },
      "source": [
        "#Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "VTmdvfLSI6DW",
        "outputId": "1fea1eb3-3d70-4faf-8350-1cf1cbd830df"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1346194419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "steps = len(descriptions) // batch_size\n",
        "\n",
        "# Filter descriptions to only include keys present in features\n",
        "filtered_descriptions = {key: descriptions[key] for key in features.keys() if key in descriptions}\n",
        "\n",
        "generator = data_generator(filtered_descriptions, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "history = model.fit(generator, epochs=20, steps_per_epoch=steps, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEV7astemy9d"
      },
      "source": [
        "#Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRd9p0Uyz8yC"
      },
      "outputs": [],
      "source": [
        "model.save('image_caption_generator.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9rp90VBmtoG"
      },
      "source": [
        "#Caption Generation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqrr7PXylLlt"
      },
      "outputs": [],
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def generate_caption(model, tokenizer, photo_feature, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo_feature.reshape(1,2048), sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BirBMnxNm3TE"
      },
      "source": [
        "#Test on a Sample Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2gtZ6xdm5W3"
      },
      "outputs": [],
      "source": [
        "# Pick an image key from test set (or any image you have features for)\n",
        "sample_img_id = list(features.keys())[0]\n",
        "sample_feature = features[sample_img_id]\n",
        "\n",
        "caption = generate_caption(model, tokenizer, sample_feature, max_length)\n",
        "print(\"Generated Caption:\", caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddb41cb5"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the features to a file\n",
        "with open('features.pkl', 'wb') as f:\n",
        "    pickle.dump(features, f)\n",
        "\n",
        "print(\"Image features saved to features.pkl\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JOJ6hxfXySsP",
        "zc2u73sqybGs",
        "3b4cuRTFyeg_",
        "cYerKa-YztzC",
        "8HsImn2N0Aqk",
        "kdHTEJvS3kHO",
        "1QKRY_S_3lwJ",
        "XSW9Rjre3tsx",
        "A_hcnPva3zb6",
        "g9MzcCY933OX",
        "ENT4_v1U49J2",
        "Lhgt4q3B6rY9",
        "y2hGSw5V57yg",
        "Wv3b_OMs_Ct2",
        "FXnJHe1e99Qj",
        "pFOwue1T_Kks",
        "syULHTcK_PVZ",
        "urK8KmeKJM6l",
        "W1_ohs9AJOki"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}